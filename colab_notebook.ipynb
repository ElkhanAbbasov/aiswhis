{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07df50b",
   "metadata": {},
   "source": [
    "# Turkish XTTS Fine-tuning on Google Colab\n",
    "\n",
    "This notebook guides you through fine-tuning XTTSv2 for Turkish language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ce707",
   "metadata": {},
   "source": [
    "## Cell 1: Check GPU\n",
    "\n",
    "Verify L4 GPU is active. Expected output should show \"NVIDIA L4\" with ~24GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416279f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify L4 GPU is active\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71db499",
   "metadata": {},
   "source": [
    "## Cell 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aeca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA 12.1\n",
    "!pip install --upgrade pip\n",
    "!pip install \"torch==2.3.1\" \"torchaudio==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install Coqui TTS and trainer\n",
    "!pip install \"TTS==0.22.0\" \"coqui-tts-trainer\"\n",
    "\n",
    "# Configure safe globals\n",
    "import torch\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import XttsAudioConfig, XttsArgs\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "\n",
    "torch.serialization.add_safe_globals([\n",
    "    XttsConfig, XttsAudioConfig, BaseDatasetConfig, XttsArgs\n",
    "])\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0315308",
   "metadata": {},
   "source": [
    "## Cell 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your dataset path\n",
    "DATASET_ROOT = \"/content/drive/MyDrive/xtts_tr_dataset\"\n",
    "METADATA_FILE = f\"{DATASET_ROOT}/metadata.txt\"\n",
    "\n",
    "# Quick check\n",
    "import os\n",
    "print(\"Wav count:\", len([f for f in os.listdir(f\"{DATASET_ROOT}/wavs\") if f.endswith(\".wav\")]))\n",
    "print(\"Metadata exists:\", os.path.isfile(METADATA_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737efd9",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Dataset Structure Required\n",
    "\n",
    "Before proceeding, make sure your Google Drive has this structure:\n",
    "\n",
    "```\n",
    "MyDrive/\n",
    "  â””â”€â”€ xtts_tr_dataset/\n",
    "      â”œâ”€â”€ metadata.txt          â† One line per audio file\n",
    "      â””â”€â”€ wavs/\n",
    "          â”œâ”€â”€ 0001.wav\n",
    "          â”œâ”€â”€ 0002.wav\n",
    "          â”œâ”€â”€ 0003.wav\n",
    "          â””â”€â”€ ... (more files)\n",
    "```\n",
    "\n",
    "**metadata.txt format** (pipe-separated):\n",
    "```\n",
    "FILE_ID|TRANSCRIPTION|TRANSCRIPTION\n",
    "```\n",
    "\n",
    "Example:\n",
    "```\n",
    "0001|Merhaba, benim adÄ±m AyÅŸe.|Merhaba, benim adÄ±m AyÅŸe.\n",
    "0002|BugÃ¼n hava Ã§ok gÃ¼zel.|BugÃ¼n hava Ã§ok gÃ¼zel.\n",
    "```\n",
    "\n",
    "âš ï¸ **Important**: \n",
    "- File IDs in metadata.txt must match WAV filenames (without .wav extension)\n",
    "- All audio should be mono, 22050 Hz, 16-bit PCM\n",
    "- Each audio clip should be 3-15 seconds long\n",
    "- Use clear, consistent speaker voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Validate your dataset structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_ROOT = \"/content/drive/MyDrive/xtts_tr_dataset\"\n",
    "METADATA_FILE = f\"{DATASET_ROOT}/metadata.txt\"\n",
    "WAVS_DIR = f\"{DATASET_ROOT}/wavs\"\n",
    "\n",
    "print(\"ðŸ” Checking dataset structure...\\n\")\n",
    "\n",
    "# Check if folders exist\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    print(f\"âŒ Dataset folder not found: {DATASET_ROOT}\")\n",
    "    print(\"   Please create it in your Google Drive\")\n",
    "elif not os.path.exists(WAVS_DIR):\n",
    "    print(f\"âŒ Wavs folder not found: {WAVS_DIR}\")\n",
    "    print(\"   Please create a 'wavs' subfolder\")\n",
    "elif not os.path.exists(METADATA_FILE):\n",
    "    print(f\"âŒ metadata.txt not found: {METADATA_FILE}\")\n",
    "    print(\"   Please create metadata.txt with your transcriptions\")\n",
    "else:\n",
    "    # Count files\n",
    "    wav_files = [f for f in os.listdir(WAVS_DIR) if f.endswith(\".wav\")]\n",
    "    \n",
    "    # Read metadata\n",
    "    with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        metadata_lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"âœ… Dataset structure looks good!\")\n",
    "    print(f\"   ðŸ“ Dataset root: {DATASET_ROOT}\")\n",
    "    print(f\"   ðŸ”Š WAV files found: {len(wav_files)}\")\n",
    "    print(f\"   ðŸ“„ Metadata entries: {len(metadata_lines)}\")\n",
    "    \n",
    "    # Check for mismatches\n",
    "    metadata_ids = set()\n",
    "    for line in metadata_lines:\n",
    "        parts = line.split('|')\n",
    "        if len(parts) >= 2:\n",
    "            metadata_ids.add(parts[0])\n",
    "    \n",
    "    wav_ids = set(Path(f).stem for f in wav_files)\n",
    "    \n",
    "    missing_wavs = metadata_ids - wav_ids\n",
    "    missing_metadata = wav_ids - metadata_ids\n",
    "    \n",
    "    if missing_wavs:\n",
    "        print(f\"\\nâš ï¸  Warning: {len(missing_wavs)} files in metadata.txt but no WAV file:\")\n",
    "        for fid in list(missing_wavs)[:5]:\n",
    "            print(f\"      - {fid}.wav is missing\")\n",
    "    \n",
    "    if missing_metadata:\n",
    "        print(f\"\\nâš ï¸  Warning: {len(missing_metadata)} WAV files have no metadata entry:\")\n",
    "        for fid in list(missing_metadata)[:5]:\n",
    "            print(f\"      - {fid}.wav needs a line in metadata.txt\")\n",
    "    \n",
    "    if not missing_wavs and not missing_metadata:\n",
    "        print(f\"\\nðŸŽ‰ Perfect! All files match. Ready to train!\")\n",
    "        print(f\"\\nðŸ“Š Sample metadata entries:\")\n",
    "        for line in metadata_lines[:3]:\n",
    "            print(f\"   {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41118703",
   "metadata": {},
   "source": [
    "## Cell 4: Create Training Script\n",
    "\n",
    "Run the cell below to create `train_gpt_xtts_tr.py` directly in Colab. This uses the `%%writefile` magic command to write the entire training script to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83809db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_gpt_xtts_tr.py\n",
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# --------------------\n",
    "# Logging / output\n",
    "# --------------------\n",
    "RUN_NAME = \"XTTSv2_Turkish_FT\"\n",
    "PROJECT_NAME = \"XTTS_trainer\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "OUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"run_tr_tr\", \"training\")\n",
    "\n",
    "# --------------------\n",
    "# Training parameters\n",
    "# --------------------\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True\n",
    "START_WITH_EVAL = True\n",
    "BATCH_SIZE = 4           # adjust if you OOM\n",
    "GRAD_ACUMM_STEPS = 64    # ensure BATCH_SIZE * GRAD_ACUMM_STEPS >= ~256\n",
    "\n",
    "# --------------------\n",
    "# Dataset config (Turkish, LJSpeech-style formatter)\n",
    "# --------------------\n",
    "DATASET_ROOT = os.environ.get(\"XTTS_TR_DATASET_ROOT\", \"/content/drive/MyDrive/xtts_tr_dataset\")\n",
    "METADATA_FILE = os.environ.get(\"XTTS_TR_METADATA_FILE\", os.path.join(DATASET_ROOT, \"metadata.txt\"))\n",
    "\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    dataset_name=\"turkish_single_speaker\",\n",
    "    path=DATASET_ROOT,\n",
    "    meta_file_train=METADATA_FILE,\n",
    "    language=\"tr\",\n",
    ")\n",
    "\n",
    "DATASETS_CONFIG_LIST = [config_dataset]\n",
    "\n",
    "# --------------------\n",
    "# Checkpoint paths (XTTS v2 + DVAE)\n",
    "# --------------------\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2_tr_base\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK       = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE   = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "TOKENIZER_FILE_LINK   = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK  = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))\n",
    "\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK],\n",
    "        CHECKPOINTS_OUT_PATH,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "# --------------------\n",
    "# Some short Turkish eval sentences for monitoring\n",
    "# --------------------\n",
    "SPEAKER_REFERENCE = [\n",
    "    # any 3â€“6 sec clear samples from your dataset:\n",
    "    # You can leave this empty; XTTS will still train, these are just for periodic audio eval.\n",
    "]\n",
    "\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "# --------------------\n",
    "# Early stopping (simple)\n",
    "# --------------------\n",
    "BEST_LOSS = None\n",
    "MAX_PATIENCE = 1\n",
    "CURRENT_PATIENCE = 0\n",
    "\n",
    "def early_stopping_fn(eval_results):\n",
    "    global BEST_LOSS, CURRENT_PATIENCE\n",
    "    print(\" > Early stopping hook\")\n",
    "    current_best_loss = eval_results.best_loss[\"eval_loss\"]\n",
    "\n",
    "    if BEST_LOSS is None:\n",
    "        BEST_LOSS = current_best_loss\n",
    "        return False\n",
    "\n",
    "    if current_best_loss < BEST_LOSS:\n",
    "        BEST_LOSS = current_best_loss\n",
    "        CURRENT_PATIENCE = 0\n",
    "        return False\n",
    "\n",
    "    CURRENT_PATIENCE += 1\n",
    "    if CURRENT_PATIENCE >= MAX_PATIENCE:\n",
    "        print(\" > Early stopping triggered\")\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# --------------------\n",
    "# Main\n",
    "# --------------------\n",
    "def main():\n",
    "    # model args (from XTTS v2 recipe, unchanged)\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=132300,  # 6s\n",
    "        min_conditioning_length=66150,   # 3s\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=255995,           # ~11.6s\n",
    "        max_text_length=300,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "\n",
    "    audio_config = XttsAudioConfig(\n",
    "        sample_rate=22050,\n",
    "        dvae_sample_rate=22050,\n",
    "        output_sample_rate=24000,\n",
    "    )\n",
    "\n",
    "    config = GPTTrainerConfig(\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"GPT XTTS v2 Turkish fine-tuning\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=48,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=4,\n",
    "        eval_split_max_size=256,\n",
    "        print_step=50,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=10000,\n",
    "        save_n_checkpoints=1,\n",
    "        save_checkpoints=True,\n",
    "        print_eval=True,\n",
    "        optimizer=\"AdamW\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=5e-6,\n",
    "        lr_scheduler=\"MultiStepLR\",\n",
    "        lr_scheduler_params={\n",
    "            \"milestones\": [50000 * 18, 150000 * 18, 300000 * 18],\n",
    "            \"gamma\": 0.5,\n",
    "            \"last_epoch\": -1,\n",
    "        },\n",
    "        test_sentences=[\n",
    "            {\n",
    "                \"text\": \"Merhaba, bu model TÃ¼rkÃ§e konuÅŸmak iÃ§in ince ayarlanmÄ±ÅŸtÄ±r.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Uzun bir paragraf yerine daha doÄŸal kÄ±sa cÃ¼mleler kullanmak daha iyidir.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "        ],\n",
    "        eval_split_size=0.05,\n",
    "    )\n",
    "\n",
    "    # Init XTTS GPT trainer model\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "    # Load dataset\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        eval_split_max_size=config.eval_split_max_size,\n",
    "        eval_split_size=config.eval_split_size,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "            # early_stopping_fn=early_stopping_fn,  # optional hook\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "\n",
    "    trainer.fit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530f5d4",
   "metadata": {},
   "source": [
    "## Cell 5: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43213457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"XTTS_TR_DATASET_ROOT\"] = \"/content/drive/MyDrive/xtts_tr_dataset\"\n",
    "os.environ[\"XTTS_TR_METADATA_FILE\"] = \"/content/drive/MyDrive/xtts_tr_dataset/metadata.txt\"\n",
    "\n",
    "# Start training\n",
    "!python train_gpt_xtts_tr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef8d0a",
   "metadata": {},
   "source": [
    "## Cell 6: Monitor Training (Optional)\n",
    "\n",
    "Run this in a separate cell while training is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./run_tr_tr/training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c698b9",
   "metadata": {},
   "source": [
    "## Cell 7: Test Inference After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Find latest run dir\n",
    "run_root = \"/content/run_tr_tr/training\"\n",
    "runs = sorted(glob.glob(os.path.join(run_root, \"XTTSv2_Turkish_FT-*\")))\n",
    "assert runs, \"No runs found\"\n",
    "model_dir = runs[-1]\n",
    "\n",
    "model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "config_path = os.path.join(model_dir, \"config.json\")\n",
    "\n",
    "print(\"Using model:\", model_path)\n",
    "\n",
    "# Initialize TTS\n",
    "tts = TTS(\n",
    "    model_path=model_path,\n",
    "    config_path=config_path,\n",
    "    gpu=True,\n",
    ")\n",
    "\n",
    "# Generate sample\n",
    "speaker_wav = \"/content/drive/MyDrive/xtts_tr_dataset/wavs/0001.wav\"\n",
    "\n",
    "tts.tts_to_file(\n",
    "    text=\"Merhaba, ben senin TÃ¼rkÃ§e konuÅŸan yapay zeka asistanÄ±nÄ±m.\",\n",
    "    file_path=\"sample_tr.wav\",\n",
    "    speaker_wav=speaker_wav,\n",
    "    language=\"tr\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Generated: sample_tr.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81cfcf",
   "metadata": {},
   "source": [
    "## Cell 8: Listen to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"sample_tr.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98519a",
   "metadata": {},
   "source": [
    "## Cell 9: Generate More Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe25cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"BugÃ¼n hava Ã§ok gÃ¼zel.\",\n",
    "    \"NasÄ±l yardÄ±mcÄ± olabilirim?\",\n",
    "    \"TÃ¼rkÃ§e konuÅŸmak iÃ§in ince ayarlanmÄ±ÅŸ bir modelim.\",\n",
    "    \"Yapay zeka teknolojisi hÄ±zla geliÅŸiyor.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_sentences):\n",
    "    output_file = f\"test_{i+1}.wav\"\n",
    "    tts.tts_to_file(\n",
    "        text=text,\n",
    "        file_path=output_file,\n",
    "        speaker_wav=speaker_wav,\n",
    "        language=\"tr\",\n",
    "    )\n",
    "    print(f\"âœ“ Generated: {output_file}\")\n",
    "    display(Audio(output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3835f",
   "metadata": {},
   "source": [
    "## Cell 10: Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce096f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the trained model\n",
    "!zip -r trained_model.zip {model_dir}\n",
    "\n",
    "# Download to your computer\n",
    "from google.colab import files\n",
    "files.download('trained_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef2ffe",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "1. **Save checkpoints to Drive**: Modify `OUT_PATH` in training script to save to Drive\n",
    "2. **Resume training**: Set `restore_path` to your checkpoint\n",
    "3. **Monitor loss**: Loss should decrease to ~1.5-2.5 for good quality\n",
    "4. **Adjust batch size**: If OOM, reduce `BATCH_SIZE` in script\n",
    "\n",
    "## Expected Timeline\n",
    "\n",
    "- Setup: 5-10 minutes\n",
    "- Dataset upload: 10-30 minutes (depending on size)\n",
    "- Training: 4-8 hours for 2h dataset\n",
    "- Testing: 2-5 minutes\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### GPU Not Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10687547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reconnect to L4\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a863928",
   "metadata": {},
   "source": [
    "### Training Too Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU usage\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9f56e",
   "metadata": {},
   "source": [
    "### OOM Error\n",
    "\n",
    "Reduce batch size in `train_gpt_xtts_tr.py`:\n",
    "```python\n",
    "BATCH_SIZE = 2  # or 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Pro tip**: Keep the Colab tab active and check progress every hour. Training can take 4-8 hours."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
